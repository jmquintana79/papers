{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0186732",
   "metadata": {},
   "source": [
    "# Dev: Papers IDs collector\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- [arXiv.org](https://arxiv.org/)\n",
    "- [arXiv API-Homepage](https://pypi.org/project/arxiv/)\n",
    "- [arXiv API-Documentation](http://lukasschwab.me/arxiv.py/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f83239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed104d78",
   "metadata": {},
   "source": [
    "## arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db6667b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_categories = ['math.ST', 'stat.ME', 'stat.AP', 'stat.CO', 'cs.LG', 'stat.ML', 'cs.AI']\n",
    "folder_output = 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c08b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url: Computer Science (cs)\n",
    "url_cs = 'https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=&terms-0-field=title&classification-computer_science=y&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=200&order=-announced_date_first&start=0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c52f2",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e4e3f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if two list of categories have any common element\n",
    "def is_category(cat_paper:list, cat_required: list):\n",
    "    cat_inter = list(set(cat_paper) & set(cat_required))\n",
    "    return len(cat_inter) > 0\n",
    "\n",
    "\n",
    "## parse papers information in a advanced query search\n",
    "def parser_page(ulr:str, verbose:bool = False)->pd.DataFrame:\n",
    "    # initialize\n",
    "    col_x = ['paper_id', 'categories', 'submission_date', 'title', 'abstract']\n",
    "\n",
    "    # download html content\n",
    "    try:\n",
    "        # get request\n",
    "        reqs = requests.get(url)\n",
    "        # get html page\n",
    "        soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f'[error] It was not possible download the html content of this url: \"{url}\"')\n",
    "            return pd.DataFrame(columns = col_x)\n",
    "\n",
    "    # initialize\n",
    "    records = list() \n",
    "\n",
    "    # loop of results\n",
    "    for tag in soup.find_all(\"li\", {\"class\": \"arxiv-result\"}):\n",
    "\n",
    "        # parse categories\n",
    "        tag_d = tag.find_all(\"div\", {\"class\": \"tags is-inline-block\"})[0]\n",
    "        categories = [it.rstrip().lstrip() for it in tag_d.text.split('\\n') if it != '']\n",
    "        if verbose:\n",
    "            print(\"\\n{0}: {1}\".format(tag_d.name, categories))   \n",
    "\n",
    "        # only continue if is a required paper by category\n",
    "        if is_category(categories, required_categories):\n",
    "\n",
    "            # parse paper id\n",
    "            tag_a = tag.find_all(\"a\")[0]\n",
    "            paper_id = tag_a.text.replace('arXiv:', '').rstrip().lstrip()\n",
    "            if verbose:\n",
    "                print(\"{0}: {1}\".format(tag_a.name, paper_id))\n",
    "\n",
    "            # parse submission date\n",
    "            tag_p = tag.find_all(\"p\")[4]\n",
    "            sdate = tag_p.text.split(';')[0].replace('Submitted', '').rstrip().lstrip()\n",
    "            dt = datetime.strptime(sdate, '%d %B, %Y')\n",
    "            submission_date = date(dt.year, dt.month, dt.day)\n",
    "            if verbose:\n",
    "                print(\"{0}: {1}\".format(tag_p.name, submission_date))\n",
    "\n",
    "            # parse title\n",
    "            tag_p = tag.find_all(\"p\")[1]\n",
    "            title = tag_p.text.replace('\\n','').rstrip().lstrip()\n",
    "            if verbose:\n",
    "                print(\"{0}: {1}\".format(tag_p.name, title))\n",
    "\n",
    "            # parse abstract\n",
    "            tag_s = tag.find_all(\"span\", {\"class\": \"abstract-full has-text-grey-dark mathjax\"})[0]\n",
    "            abstract = tag_s.contents[0].replace('\\n','').rstrip().lstrip()\n",
    "            if verbose:\n",
    "                print(\"{0}: {1}\".format(tag_s.name, abstract))\n",
    "\n",
    "            # append if any common cat\n",
    "            records.append([paper_id, categories, submission_date, title, abstract])\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"discarted because is not required.\")\n",
    "\n",
    "    # store in a df\n",
    "    df = pd.DataFrame(records, columns = col_x)\n",
    "    if verbose:\n",
    "        print(f'\\nFinally was parsed {len(df)} papers.')\n",
    "\n",
    "    # return\n",
    "    return df\n",
    "\n",
    "\n",
    "## get size of page\n",
    "def get_page_size(url:str)->int:\n",
    "    try:\n",
    "        return int([iu for iu in url.split('&') if 'size' in iu][0].replace('size=', ''))\n",
    "    except:\n",
    "        print('[error] It is not available \"size\" tag in this url.')\n",
    "        return None\n",
    "\n",
    "    \n",
    "## get next page\n",
    "def next_paginate(url:str, size:int)->str:\n",
    "    try:\n",
    "        start = int([iu for iu in url.split('&') if 'start' in iu][0].replace('start=', ''))\n",
    "        return url.replace(f'start={start}', f'start={start + size}')\n",
    "    except:\n",
    "        print('[error] It is not available \"start\" tag in this url.')\n",
    "        return None\n",
    "\n",
    "    \n",
    "## check if url is valid\n",
    "def is_valid_url(url:str)->bool:\n",
    "    if 'size' in url and 'start=0' in url and '=all' in url and 'abstracts=show' in url and 'include_cross_list=include' in url:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31220b03",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c2ceafbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a valid url.\n"
     ]
    }
   ],
   "source": [
    "# select url\n",
    "url = url_cs\n",
    "# check if a valid url\n",
    "if is_valid_url(url):\n",
    "    print('It is a valid url.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dc1f0aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Page 1 - total num records = 75\n",
      "--> Page 2 - total num records = 172\n"
     ]
    }
   ],
   "source": [
    "# maximum number of pages\n",
    "max_num_pages = 2\n",
    "# get page size\n",
    "size = get_page_size(url)\n",
    "# initialize\n",
    "num_page = 1\n",
    "# loop\n",
    "while num_page <= max_num_pages:\n",
    "    # parse\n",
    "    idf = parser_page(url, verbose = False)\n",
    "    # validate\n",
    "    if len(idf)== 0:\n",
    "        print('Stop loop!')\n",
    "        break\n",
    "    else:\n",
    "        # append\n",
    "        if num_page == 1:\n",
    "            df = idf.copy()\n",
    "        else:\n",
    "            df = df.append(idf)\n",
    "        # display\n",
    "        print(f'--> Page {num_page} - total num records = {len(df)}')\n",
    "        # clean\n",
    "        del idf\n",
    "        # get next page url\n",
    "        url = next_paginate(url, size)\n",
    "    # add counter\n",
    "    num_page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6d53d2a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>submission_date</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2109.08141</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG]</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>An End-to-End Transformer Model for 3D Object ...</td>\n",
       "      <td>We propose 3DETR, an end-to-end Transformer ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2109.08139</td>\n",
       "      <td>[eess.SP, cs.LG, cs.NI, stat.ML]</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>Adversarial Attacks against Deep Learning Base...</td>\n",
       "      <td>We consider adversarial machine learning based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2109.08134</td>\n",
       "      <td>[cs.LG, stat.ML]</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>Comparison and Unification of Three Regulariza...</td>\n",
       "      <td>In batch reinforcement learning, there can be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2109.08131</td>\n",
       "      <td>[cs.HC, cs.CY, cs.LG]</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>Studying Up Machine Learning Data: Why Talk Ab...</td>\n",
       "      <td>Research in machine learning (ML) has primaril...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2109.08128</td>\n",
       "      <td>[cs.LG, cs.AI, cs.RO]</td>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>Conservative Data Sharing for Multi-Task Offli...</td>\n",
       "      <td>Offline reinforcement learning (RL) algorithms...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id                        categories submission_date  \\\n",
       "0  2109.08141             [cs.CV, cs.AI, cs.LG]      2021-09-16   \n",
       "1  2109.08139  [eess.SP, cs.LG, cs.NI, stat.ML]      2021-09-16   \n",
       "2  2109.08134                  [cs.LG, stat.ML]      2021-09-16   \n",
       "3  2109.08131             [cs.HC, cs.CY, cs.LG]      2021-09-16   \n",
       "4  2109.08128             [cs.LG, cs.AI, cs.RO]      2021-09-16   \n",
       "\n",
       "                                               title  \\\n",
       "0  An End-to-End Transformer Model for 3D Object ...   \n",
       "1  Adversarial Attacks against Deep Learning Base...   \n",
       "2  Comparison and Unification of Three Regulariza...   \n",
       "3  Studying Up Machine Learning Data: Why Talk Ab...   \n",
       "4  Conservative Data Sharing for Multi-Task Offli...   \n",
       "\n",
       "                                            abstract  \n",
       "0  We propose 3DETR, an end-to-end Transformer ba...  \n",
       "1  We consider adversarial machine learning based...  \n",
       "2  In batch reinforcement learning, there can be ...  \n",
       "3  Research in machine learning (ML) has primaril...  \n",
       "4  Offline reinforcement learning (RL) algorithms...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ca6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649fe68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papers",
   "language": "python",
   "name": "papers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
